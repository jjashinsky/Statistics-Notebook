---
title: "Simple Linear Regression"
output: 
  html_document:
    theme: cerulean
    code_folding: hide
---
```{r, include=FALSE}
library(plotrix)
library(mosaic)
library(pander)
library(DT)

points <- c(1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000)
comp.time <- c(.1488, .1417, .1784, .1611, .1871, .2145, .2431, .2663, .2907, .4133, .3504, .3721, .3974, .4311, .4636, .4947, .5037, .5335, .5620, .5838, .8088, 1.0570, 1.3399, 1.8432, 1.8150, 2.0642, 2.2960, 2.7075)
time <- data.frame(points,comp.time)
```

<script type="text/javascript">
 function showhide(id) {
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
 }
</script>


### Background

Computers are incredibly fast at computing algorithms and numbers. The heavier the calculation the more time it takes to compute it. My goal was to see if I could accurately predict the time it takes for a computer to estimate pi, or in other words, is there a linear relationship between number of computations and time. 

The function `monte.carlo.pi(n)` was used, where n is any desired number of points. (Press on the code button below to view how this function was defined) The goal of the function is to show that random probability can be used to estimate pi. It does this by randomly plotting n number of points in a square with a circle circumscribed within the square. Some points happen to land outside the circle and some land inside the circle. 

A ratio of the points inside the circle to the total points should be approximately equal to the ratio of the area of the circle to the area of the square. After some simplification of algebra we can see rewrite the ratio as,  

$$
\pi \approx 4\times ( \, \frac{\text{Number of Points in Circle}}{\text{Number of Points in the Square}}  ) \,
$$
The function will determine the number of points within the circle and divide it by the total number of points, n, and then multiply it by 4. This will approximate $\pi$. 

An example of this graphic and the function's output is shown below.

----

```{r}
monte.carlo.pi<-function(n)
{
  start.time <- Sys.time()
  circle.points<-0
  square.points<-0
  x<-runif(n,0,1)
  y<-runif(n,0,1)
  z<-rep(0, n)
  for (i in 1:n)
  {
    if ((x[i]-.5)^2 + (y[i]-.5)^2 <=.5^2)
    {
      circle.points<-circle.points+1
      square.points<-square.points+1
      z[i]<-1
    } else
    {
      square.points<-square.points+1
    }
  }
  plot.new()
  frame()
  plot(x,y,asp=1,xlim=c(0,1),ylim=c(0,1), pch=16, col=(c("blue","green")[as.factor(z)]), cex = 4/log(n))
  draw.circle(0.5,0.5,1/2,nv=1000,border=NULL,col=NA,lty=1,lwd=1)
  rect(0,0,1,1)
  cat("Estimate of pi: ", 4.0 * circle.points / square.points,"\n")
  cat("Computed in:    ", Sys.time() - start.time, "sec\n")
  #return()
}

monte.carlo.pi(1000)
```

----

The function also keeps track of how much time it takes to run through all of the code. **Generally speaking, as the number of points used in the function increases the computation time will also increase**. 

My question is whether there exists a linear relationship between number of points and computation time. I will not be looking at the actual calculation of the monte.carlo.pi function or the accuracy of the estimation. The only variables this analysis will look at is the relationship between the number of points used and the speed at which $\pi$ was estimated.  

A sample of 20 computations was done starting at 1,000 points with 1,000 increments. After reaching 20,000 the increments increased to 10,000 until 100,000 points were reached. The final sample size is 28. Each number was rounded to four decimals. 

<a href="javascript:showhide('long')">The Data <span style="font-size:8pt;">(click to view)</span></a>


<div id="long" style="display:none;">
```{r}
knitr::kable(time)
```

</div>


### Hypothesis

The analysis will only look at the slope of the linear line, therefore the hypothesis is written as thus,

$$
H_0: \beta_1=0
$$

$$
H_a: \beta_1 \neq 0
$$
$$
\alpha = 0.05
$$

### Analysis 

```{r}
ggplot(data = time, aes(x = points, y = comp.time)) + 
  geom_point(color='black') +
  geom_smooth(method = "lm", se = FALSE, color="red") +
  xlab("Number of Points used in estimation") +
  ylab("Computation time (seconds)") +
  ggtitle("Linear Relationship of points and Computation Time")
```

It appears through the plot that there is a linear relationship. The results of the test are shown below.

```{R}
mylm <- lm(comp.time ~ points, data=time)
pander(summary(mylm))
```

Comparing our p-value to alpha we get $(2.191e-30 < 0.05)$. The results lead us to **reject the null hypothesis** and conclude with the alternative. There is in fact a linear relationship between the number of points used and computation time. 

To make sure this test was appropriate these 5 assumptions should be checked. 

1. Linear Relation: the regression relation between $Y$ and $X$ is linear.

2. Normal Errors: the error terms are normally distributed with a mean of zero.

3. Constant Variance: the variance of the error terms is constant overall $X$ values.

4. Fixed X: the $X$ values can be considered fixed and measured without error.

5. Independent Errors: the error terms are independent.

Below is a plot of the residuals and a QQ plot of the error terms. 

```{r}
plot(mylm, which=1:2)
plot(mylm$residuals, main="Residuals vs Order", xlab="",
     ylab="Residuals")
```

The first plot has obvious signs of trends. This would not support assumptions 1 and 3. The second plot shows the normality of our error terms. This too shows signs that error terms are not normally distributed, meaning the second assumption has not been met. But we can assume that the fourth has been met.

#### Transformations 

In an effort to make these assumptions true, I made an attempt to transform the data. The tranformations done were the natural log, square root and squaring of speed.

The graphs below check the assumptions 1, 2, and 3 with the transformed point values.  

```{r}
par(mfrow=c(1,2))
log.x <- log(points, base = exp(1))
log.lm <- lm(comp.time ~ log.x)
plot(log.lm, which=1:2, main="Natural Log of points")
```


```{r}
par(mfrow=c(1,2))
sqrt.x <- (points)^(1/2)
sqrt.lm <- lm(comp.time ~ sqrt.x)
plot(sqrt.lm, which=1:2, main="Square Root of points")
```


```{r}
par(mfrow=c(1,2))
sqr.x <- (points)^2
sqr.lm <- lm(comp.time ~ sqr.x)
plot(sqr.lm, which=1:2, main="Square of points")
```

We can see that the transformations did not help make these assumptions true. Therefore this analysis will only use and refer to the original data set as before mentioned. 

### Interpretation 

Although the analysis found a very strong correlation, these results could be argued on the fact that assumptions 1, 2, 3, and 5 were not successfully met. Also, a linear model may be successful in predicting computation time, but it may begin to fail when the number of points reaches far above the numbers used in the present data set. As well, there may be other factors at play here, such as whether a background program on the computer used an inconsistent amount of processing power or memory during the time I was collecting samples. 

Future research could be done by replicating the data. Considering that only one sample was taken from each increment it might be making it difficult for the linear regression to accurately predict the line of best fit. Another aspect that could reasonably be done is to extend the data to include points of much greater values. The analysis could also be expanded to a multiple linear regression and include the factor of accuracy; whether the approximation was greater or less then pi. All of these suggestions will increase sample size and hopefully improve the success of the assumptions and improve our confidence in the results. 

----


